{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPU_trained.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0T9hdnQ8Yuru",
        "outputId": "a19f730f-bc48-44f1-9380-0e7238dd57df"
      },
      "source": [
        "!wget https://cs.stanford.edu/people/karpathy/deepimagesent/flickr8k.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-17 19:49:27--  https://cs.stanford.edu/people/karpathy/deepimagesent/flickr8k.zip\n",
            "Resolving cs.stanford.edu (cs.stanford.edu)... 171.64.64.64\n",
            "Connecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 50286882 (48M) [application/zip]\n",
            "Saving to: ‘flickr8k.zip’\n",
            "\n",
            "flickr8k.zip        100%[===================>]  47.96M  14.0MB/s    in 3.7s    \n",
            "\n",
            "2021-11-17 19:49:31 (12.9 MB/s) - ‘flickr8k.zip’ saved [50286882/50286882]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WKIBANtZP5N",
        "outputId": "388d4735-ae73-463a-9734-25f3ff8f5f63"
      },
      "source": [
        "!unzip flickr8k.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  flickr8k.zip\n",
            "  inflating: flickr8k/dataset.json   \n",
            "  inflating: flickr8k/readme.txt     \n",
            "  inflating: flickr8k/vgg_feats.mat  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9BCV74Jc-ny"
      },
      "source": [
        "!mkdir train\n",
        "!mkdir val"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRSyqmJHb9cI"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torchvision.models.vgg as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import json\n",
        "import pickle\n",
        "import scipy.io"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmJNpd14aaSq"
      },
      "source": [
        "###Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEBPu6hMZTmz",
        "outputId": "e7487198-7e63-4c9f-f782-533f08f2468d"
      },
      "source": [
        "\n",
        "\n",
        "dataset_path = 'flickr8k'\n",
        "\n",
        "#Images\n",
        "image_data = scipy.io.loadmat(dataset_path + '/vgg_feats.mat')\n",
        "images = image_data['feats']\n",
        "\n",
        "#Captions\n",
        "with open(dataset_path + '/dataset.json') as f:\n",
        "\tcaption_data = json.load(f)['images']\n",
        "\n",
        "S = images.shape[1]\n",
        "T = int(S * 0.9)\n",
        "V = S - T\n",
        "\n",
        "train_imgs = np.zeros((T, 4096))\n",
        "train_caps = []\n",
        "train_names = []\n",
        "\n",
        "val_imgs = np.zeros((V, 4096))\n",
        "val_caps = []\n",
        "val_names = []\n",
        "\n",
        "for image_id in range(images.shape[1]):\n",
        "    if image_id < T:\n",
        "        train_imgs[image_id] = images[:, image_id]\n",
        "        train_names.append(caption_data[image_id]['filename'])\n",
        "        for i in range(5):\n",
        "            train_caps.append(caption_data[image_id]['sentences'][i]['raw'])\n",
        "    else:\n",
        "        val_imgs[image_id - T] = images[:, image_id]\n",
        "        val_names.append(caption_data[image_id - T]['filename'])\n",
        "        for i in range(5):\n",
        "            val_caps.append(caption_data[image_id - T]['sentences'][i]['raw'])\n",
        "\n",
        "print('Train Images shape:', train_imgs.shape, 'Train Length of captions:', len(train_caps))\n",
        "print('Train Images shape:', val_imgs.shape, 'Train Length of captions:', len(val_caps))\n",
        "\n",
        "# Saving Train\n",
        "with open('train/image_features.pkl', 'wb') as f:\n",
        "\tpickle.dump(train_imgs, f)\n",
        "with open('train/captions.pkl', 'wb') as f:\n",
        "\tpickle.dump(train_caps, f)\n",
        "with open('train/file_names.pkl', 'wb') as f:\n",
        "\tpickle.dump(train_names, f)\n",
        "\n",
        "# Saving Val\n",
        "with open('val/image_features.pkl', 'wb') as f:\n",
        "\tpickle.dump(val_imgs, f)\n",
        "with open('val/captions.pkl', 'wb') as f:\n",
        "\tpickle.dump(val_caps, f)\n",
        "with open('val/file_names.pkl', 'wb') as f:\n",
        "\tpickle.dump(val_names, f)\n",
        "\n",
        "print('Saved')\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Images shape: (7200, 4096) Train Length of captions: 36000\n",
            "Train Images shape: (800, 4096) Train Length of captions: 4000\n",
            "Saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8rzYkp8aexl"
      },
      "source": [
        "###Utilitary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qgPwA2pZyeY"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import string\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "def tokenize(text):\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\t# tokenize\n",
        "\tdesc = text.split()\n",
        "\t# to lower\n",
        "\tdesc = [word.lower() for word in desc]\n",
        "\t# remove punctuation\n",
        "\tdesc = [word.translate(table) for word in desc]\n",
        "\t# remove words less in len\n",
        "\tdesc = [word for word in desc if len(word) > 1]\n",
        "\t# remove numbers\n",
        "\tdesc = [word for word in desc if word.isalpha()]\n",
        "\treturn desc\n",
        "\n",
        "\n",
        "def build_dictionary(text):\n",
        "    \"\"\"\n",
        "    Build a dictionary (mapping of tokens to indices)\n",
        "    text: list of sentences (pre-tokenized)\n",
        "    \"\"\"\n",
        "    wordcount = {}\n",
        "    for cc in text:\n",
        "        words = tokenize(cc)\n",
        "        for word in words:\n",
        "            if word not in wordcount:\n",
        "                wordcount[word] = 0\n",
        "            wordcount[word] += 1\n",
        "\t\t# print(words)\n",
        "\n",
        "    words = list(wordcount.keys())\n",
        "    freqs = list(wordcount.values())\n",
        "    sorted_idx = np.argsort(freqs)[::-1]\n",
        "\n",
        "    worddict = {}\n",
        "    for idx, sidx in enumerate(sorted_idx):\n",
        "        worddict[words[sidx]] = idx+2  # 0: <eos>, 1: <unk>\n",
        "\n",
        "    return worddict\n",
        "\n",
        "def get_hot(cap, worddict):\n",
        "\tx = np.zeros((len(cap.split())+1, len(worddict)+2))\n",
        "\n",
        "\tr = 0\n",
        "\tfor w in cap.split():\n",
        "\t\tif w in worddict:\n",
        "\t\t\tx[r, worddict[w]] = 1\n",
        "\t\telse:\n",
        "\t\t\t# Unknown word/character\n",
        "\t\t\tx[r, 1] = 1\n",
        "\t\tr += 1\n",
        "\t# EOS\n",
        "\tx[r, 0] = 1\n",
        "\n",
        "\treturn torch.from_numpy(x).float().cuda()\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kiWtVBHaciQ"
      },
      "source": [
        "###Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJdMHbFqZu6F"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "\n",
        "# Image Encoder\n",
        "class ImageEncoder(nn.Module):\n",
        "\n",
        "\tdef __init__(self, EMBEDDING_SIZE, COMMON_SIZE):\n",
        "\t\tsuper(ImageEncoder, self).__init__()\n",
        "\t\tself.linear = nn.Linear(EMBEDDING_SIZE, COMMON_SIZE)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\treturn self.linear(x).abs()\n",
        "\n",
        "class SentencesEncoder(nn.Module):\n",
        "\n",
        "\tdef __init__(self, VOCAB_SIZE, WORD_EMBEDDING_SIZE, COMMON_SIZE):\n",
        "\t\tsuper(SentencesEncoder, self).__init__()\n",
        "\t\tself.embed = nn.Linear(VOCAB_SIZE, WORD_EMBEDDING_SIZE)\n",
        "\t\tself.encoder = nn.GRU(WORD_EMBEDDING_SIZE, COMMON_SIZE)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tx = self.embed(x)\n",
        "\t\to, h = self.encoder(x.reshape(x.shape[0], 1, x.shape[1]))\n",
        "\t\treturn h.reshape(1, -1).abs()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fG6x2eEahbE"
      },
      "source": [
        "###Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhjxHr_tewJK",
        "outputId": "da42d082-c37b-421c-915d-39d62a26a26d"
      },
      "source": [
        "dataset_path = 'flickr8k'\n",
        "\n",
        "# Loading the dataset\n",
        "print('Loading the train dataset')\n",
        "with open('train/image_features.pkl', 'rb') as f:\n",
        "\ttrain_images = pickle.load(f)\n",
        "\ttrain_images = train_images.astype(np.float32)\n",
        "\ttrain_images = train_images / torch.norm(torch.from_numpy(train_images), dim=1, p=2).reshape(-1, 1)\n",
        "\n",
        "with open('train/captions.pkl', 'rb') as f:\n",
        "\ttrain_caps = pickle.load(f)\n",
        "\t\n",
        "print('Images shape:', train_images.shape, 'Length of captions:', len(train_caps))\n",
        "\n",
        "print('Loading the val dataset')\n",
        "with open('val/image_features.pkl', 'rb') as f:\n",
        "\tval_images = pickle.load(f)\n",
        "\tval_images = val_images.astype(np.float32)\n",
        "\tval_images = val_images / torch.norm(torch.from_numpy(val_images), dim=1, p=2).reshape(-1, 1)\n",
        "\n",
        "with open('val/captions.pkl', 'rb') as f:\n",
        "\tval_caps = pickle.load(f)\n",
        "\n",
        "print('Val Images shape:', val_images.shape, 'Length of Val captions:', len(val_caps))\n",
        "\n",
        "# Creating dictionary and saving\n",
        "print('Creating the word dictionary')\n",
        "worddict = build_dictionary(train_caps)\n",
        "with open('worddict.pkl', 'wb') as f:\n",
        "\tpickle.dump(worddict, f)\n",
        "print('Dictionary size:', len(worddict))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the train dataset\n",
            "Images shape: torch.Size([7200, 4096]) Length of captions: 36000\n",
            "Loading the val dataset\n",
            "Val Images shape: torch.Size([800, 4096]) Length of Val captions: 4000\n",
            "Creating the word dictionary\n",
            "Dictionary size: 8267\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuVS3I9RdvrM"
      },
      "source": [
        "def Score(caps, imgs):\n",
        "\tz = torch.zeros(caps.shape).cuda()\n",
        "\treturn -torch.sum(torch.max(z, caps-imgs)**2, dim=1).cuda()\n",
        "\n",
        "def triplet_loss_img(anchor, positive, negative, margin):\n",
        "\tps = Score(positive, anchor)\n",
        "\tpn = Score(negative, anchor)\n",
        "\tz = torch.zeros(ps.shape).cuda()\n",
        "\treturn torch.sum(torch.max(z, margin - ps + pn))\n",
        "\n",
        "def triplet_loss_cap(anchor, positive, negative, margin):\n",
        "\tps = Score(anchor, positive)\n",
        "\tpn = Score(anchor, negative)\n",
        "\tz = torch.zeros(ps.shape).cuda()\n",
        "\treturn torch.sum(torch.max(z, margin - ps + pn))"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgDb4xHYZkHs",
        "outputId": "a2b9f7e2-d3a9-4611-98b3-e62598147bec"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "from torch import optim\n",
        "import torchvision.models.vgg as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import pickle\n",
        "import json\n",
        "\n",
        "# Parameters\n",
        "margin = 0.05\n",
        "max_epochs = 10\n",
        "dim_image = 4096\n",
        "batch_size = 256\n",
        "dim = 1024\n",
        "dim_word = 300\n",
        "lrate = 0.001\n",
        "\n",
        "# Loading models\n",
        "ImgEncoder = ImageEncoder(dim_image, dim).cuda()\n",
        "SentenceEncoder = SentencesEncoder(len(worddict)+2, dim_word, dim).cuda()\n",
        "print('Models loaded')\n",
        "\n",
        "# Adam Optimizer\n",
        "optimizer = optim.Adam(list(ImgEncoder.parameters()) + list(SentenceEncoder.parameters()), lr = lrate)\n",
        "print('Loaded Adam optimizer')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models loaded\n",
            "Loaded Adam optimizer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wg3AnDdCgNYK",
        "outputId": "61a9c2df-7149-4763-d726-2bd0bcc4ce97"
      },
      "source": [
        "# Training\n",
        "print('Training begins')\n",
        "epochLoss = []\n",
        "meanRank = []\n",
        "for epoch in range(max_epochs):\n",
        "\t\n",
        "\tprint('Epoch:', epoch)\n",
        "\ttotalLoss = 0\n",
        "\n",
        "\tfor batch_index in range(0, train_images.shape[0], batch_size):\n",
        "\t\t\n",
        "\t\tif batch_index + batch_size >= train_images.shape[0]:\n",
        "\t\t\tbreak\n",
        "\n",
        "\t\t# Data preproc\n",
        "\t\tcurr_ims = train_images[batch_index:batch_index+batch_size]\n",
        "\t\tall5_caps = train_caps[5*batch_index:5*(batch_index+batch_size)]\n",
        "\t\tcurr_caps = []\n",
        "\t\tfor i in range(batch_size):\n",
        "\t\t\tcurr_caps.append(all5_caps[5*i + np.random.randint(0, 5)])\n",
        "\n",
        "\t\tone_hot_caps = []\n",
        "\t\tfor i in range(batch_size):\n",
        "\t\t\tone_hot_caps.append(get_hot(curr_caps[i], worddict))\n",
        "\n",
        "\t\t# Encoding\n",
        "\t\tencoded_ims = ImgEncoder(curr_ims.cuda()).cuda()\n",
        "\t\tencoded_caps = []\n",
        "\t\tfor i in range(batch_size):\n",
        "\t\t\tencoded_caps.append(SentenceEncoder(one_hot_caps[i]))\n",
        "\t\tencoded_caps = torch.stack(encoded_caps).reshape(batch_size, dim)\n",
        "\n",
        "\t\t# Real training\n",
        "\t\toptimizer.zero_grad()\n",
        "\n",
        "\t\t# Calculating Loss\n",
        "\t\tloss = 0\n",
        "\t\tfor i in range(batch_size):\n",
        "\t\t\t# Image as anchor\n",
        "\t\t\tanchor = encoded_ims[i:i+1].repeat(batch_size - 1, 1)\n",
        "\t\t\tpositive = encoded_caps[i:i+1].repeat(batch_size - 1, 1)\n",
        "\t\t\tnegative = torch.cat((encoded_caps[:i], encoded_caps[i+1:]), 0)\n",
        "\t\t\tloss += triplet_loss_img(anchor, positive, negative, margin)\n",
        "\n",
        "\t\t\t# Caption as anchor\n",
        "\t\t\tanchor = encoded_caps[i:i+1].repeat(batch_size - 1, 1)\n",
        "\t\t\tpositive = encoded_ims[i:i+1].repeat(batch_size - 1, 1)\n",
        "\t\t\tnegative = torch.cat((encoded_ims[:i], encoded_ims[i+1:]), 0)\n",
        "\t\t\tloss += triplet_loss_cap(anchor, positive, negative, margin)\n",
        "\n",
        "\t\t# Logging\n",
        "\t\ttotalLoss += loss.item()\n",
        "\t\tprint('Samples seen: ' + str(batch_index+batch_size) +  '/' + str(train_images.shape[0]), 'loss:', loss.item())\n",
        "\n",
        "\t\t# Updating weights\n",
        "\t\tloss.backward()\n",
        "\t\toptimizer.step()\n",
        "\n",
        "\t# Logging for early stopping\n",
        "\tprint('Training loss:', totalLoss)\n",
        "\tepochLoss.append(totalLoss)\n",
        "\n",
        "\t# Ranks on test set\n",
        "\tr = []\n",
        "\tencoded_val_ims = ImgEncoder(val_images.cuda()).cuda()\n",
        "\tfor i in range(len(val_caps)):\n",
        "\t\thot = get_hot(val_caps[i], worddict)\n",
        "\t\tencoded_val_cap = SentenceEncoder(hot).repeat(val_images.shape[0], 1)\n",
        "\t\tS = Score(encoded_val_cap, encoded_val_ims)\n",
        "\t\tranks = S.argsort().cpu().numpy()[::-1]\n",
        "\t\tr.append(np.where(ranks==i//5)[0][0] + 1)\n",
        "\t\n",
        "\tprint('Mean rank on val set: ' + str(np.mean(np.array(r))) + '/' + str(val_images.shape[0]))\n",
        "\tmeanRank.append(np.mean(np.array(r)))\n",
        "\n",
        "# Saving models\n",
        "print(\"Training Completed!\")\n",
        "print('Loss over epochs')\n",
        "print(epochLoss)\n",
        "print('Mean rank over epochs')\n",
        "print(meanRank)\n",
        "torch.save(ImgEncoder.state_dict(), 'ImgEncoder.pt')\n",
        "torch.save(SentenceEncoder.state_dict(), 'SentenceEncoder.pt')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training begins\n",
            "Epoch: 0\n",
            "Samples seen: 256/7200 loss: 3746.064697265625\n",
            "Samples seen: 512/7200 loss: 3405.545654296875\n",
            "Samples seen: 768/7200 loss: 3528.529541015625\n",
            "Samples seen: 1024/7200 loss: 3465.2353515625\n",
            "Samples seen: 1280/7200 loss: 3222.9375\n",
            "Samples seen: 1536/7200 loss: 3286.033447265625\n",
            "Samples seen: 1792/7200 loss: 3470.75732421875\n",
            "Samples seen: 2048/7200 loss: 3384.11474609375\n",
            "Samples seen: 2304/7200 loss: 3164.72412109375\n",
            "Samples seen: 2560/7200 loss: 3148.33251953125\n",
            "Samples seen: 2816/7200 loss: 3005.796142578125\n",
            "Samples seen: 3072/7200 loss: 3014.385009765625\n",
            "Samples seen: 3328/7200 loss: 2919.254638671875\n",
            "Samples seen: 3584/7200 loss: 2896.220703125\n",
            "Samples seen: 3840/7200 loss: 2408.854248046875\n",
            "Samples seen: 4096/7200 loss: 2736.608642578125\n",
            "Samples seen: 4352/7200 loss: 2708.688720703125\n",
            "Samples seen: 4608/7200 loss: 2889.524658203125\n",
            "Samples seen: 4864/7200 loss: 2555.721435546875\n",
            "Samples seen: 5120/7200 loss: 3056.409423828125\n",
            "Samples seen: 5376/7200 loss: 2668.415771484375\n",
            "Samples seen: 5632/7200 loss: 2820.5244140625\n",
            "Samples seen: 5888/7200 loss: 2549.3017578125\n",
            "Samples seen: 6144/7200 loss: 2632.706298828125\n",
            "Samples seen: 6400/7200 loss: 2939.331298828125\n",
            "Samples seen: 6656/7200 loss: 2924.32421875\n",
            "Samples seen: 6912/7200 loss: 2427.43017578125\n",
            "Samples seen: 7168/7200 loss: 2517.431396484375\n",
            "Training loss: 83493.20385742188\n",
            "Mean rank on val set: 402.12875/800\n",
            "Epoch: 1\n",
            "Samples seen: 256/7200 loss: 2348.629638671875\n",
            "Samples seen: 512/7200 loss: 2319.059326171875\n",
            "Samples seen: 768/7200 loss: 2355.564208984375\n",
            "Samples seen: 1024/7200 loss: 2274.431640625\n",
            "Samples seen: 1280/7200 loss: 2315.2353515625\n",
            "Samples seen: 1536/7200 loss: 2260.3564453125\n",
            "Samples seen: 1792/7200 loss: 2304.845947265625\n",
            "Samples seen: 2048/7200 loss: 2260.84619140625\n",
            "Samples seen: 2304/7200 loss: 2212.57568359375\n",
            "Samples seen: 2560/7200 loss: 1991.689208984375\n",
            "Samples seen: 2816/7200 loss: 1961.325927734375\n",
            "Samples seen: 3072/7200 loss: 1981.7899169921875\n",
            "Samples seen: 3328/7200 loss: 1933.7291259765625\n",
            "Samples seen: 3584/7200 loss: 2286.380615234375\n",
            "Samples seen: 3840/7200 loss: 1892.708984375\n",
            "Samples seen: 4096/7200 loss: 1931.9794921875\n",
            "Samples seen: 4352/7200 loss: 1889.617431640625\n",
            "Samples seen: 4608/7200 loss: 1926.1357421875\n",
            "Samples seen: 4864/7200 loss: 1855.7198486328125\n",
            "Samples seen: 5120/7200 loss: 2024.631591796875\n",
            "Samples seen: 5376/7200 loss: 1910.0399169921875\n",
            "Samples seen: 5632/7200 loss: 1897.2271728515625\n",
            "Samples seen: 5888/7200 loss: 2048.389404296875\n",
            "Samples seen: 6144/7200 loss: 2094.71240234375\n",
            "Samples seen: 6400/7200 loss: 2124.450439453125\n",
            "Samples seen: 6656/7200 loss: 1932.288818359375\n",
            "Samples seen: 6912/7200 loss: 2116.025390625\n",
            "Samples seen: 7168/7200 loss: 2192.413818359375\n",
            "Training loss: 58642.79968261719\n",
            "Mean rank on val set: 401.5795/800\n",
            "Epoch: 2\n",
            "Samples seen: 256/7200 loss: 1758.5875244140625\n",
            "Samples seen: 512/7200 loss: 1873.1322021484375\n",
            "Samples seen: 768/7200 loss: 1927.552490234375\n",
            "Samples seen: 1024/7200 loss: 1835.529052734375\n",
            "Samples seen: 1280/7200 loss: 1851.754150390625\n",
            "Samples seen: 1536/7200 loss: 2036.775146484375\n",
            "Samples seen: 1792/7200 loss: 1964.0931396484375\n",
            "Samples seen: 2048/7200 loss: 1629.3363037109375\n",
            "Samples seen: 2304/7200 loss: 1984.64501953125\n",
            "Samples seen: 2560/7200 loss: 1897.910400390625\n",
            "Samples seen: 2816/7200 loss: 1615.5338134765625\n",
            "Samples seen: 3072/7200 loss: 1528.5235595703125\n",
            "Samples seen: 3328/7200 loss: 1672.6873779296875\n",
            "Samples seen: 3584/7200 loss: 1735.5928955078125\n",
            "Samples seen: 3840/7200 loss: 1582.201416015625\n",
            "Samples seen: 4096/7200 loss: 1790.6815185546875\n",
            "Samples seen: 4352/7200 loss: 1563.772705078125\n",
            "Samples seen: 4608/7200 loss: 1890.754638671875\n",
            "Samples seen: 4864/7200 loss: 1785.1612548828125\n",
            "Samples seen: 5120/7200 loss: 1955.1356201171875\n",
            "Samples seen: 5376/7200 loss: 1758.774169921875\n",
            "Samples seen: 5632/7200 loss: 1615.66455078125\n",
            "Samples seen: 5888/7200 loss: 1525.914306640625\n",
            "Samples seen: 6144/7200 loss: 1611.288818359375\n",
            "Samples seen: 6400/7200 loss: 1575.568115234375\n",
            "Samples seen: 6656/7200 loss: 1797.756103515625\n",
            "Samples seen: 6912/7200 loss: 1660.94140625\n",
            "Samples seen: 7168/7200 loss: 1448.7119140625\n",
            "Training loss: 48873.97961425781\n",
            "Mean rank on val set: 400.796/800\n",
            "Epoch: 3\n",
            "Samples seen: 256/7200 loss: 1609.7464599609375\n",
            "Samples seen: 512/7200 loss: 1790.350341796875\n",
            "Samples seen: 768/7200 loss: 1503.0887451171875\n",
            "Samples seen: 1024/7200 loss: 1547.1083984375\n",
            "Samples seen: 1280/7200 loss: 1388.268310546875\n",
            "Samples seen: 1536/7200 loss: 1594.2642822265625\n",
            "Samples seen: 1792/7200 loss: 1516.04052734375\n",
            "Samples seen: 2048/7200 loss: 1337.394775390625\n",
            "Samples seen: 2304/7200 loss: 1411.47998046875\n",
            "Samples seen: 2560/7200 loss: 1347.3128662109375\n",
            "Samples seen: 2816/7200 loss: 1300.89501953125\n",
            "Samples seen: 3072/7200 loss: 1310.2222900390625\n",
            "Samples seen: 3328/7200 loss: 1259.5726318359375\n",
            "Samples seen: 3584/7200 loss: 1531.641357421875\n",
            "Samples seen: 3840/7200 loss: 1314.0841064453125\n",
            "Samples seen: 4096/7200 loss: 1112.68994140625\n",
            "Samples seen: 4352/7200 loss: 1255.5791015625\n",
            "Samples seen: 4608/7200 loss: 1497.46630859375\n",
            "Samples seen: 4864/7200 loss: 1262.6964111328125\n",
            "Samples seen: 5120/7200 loss: 1393.2947998046875\n",
            "Samples seen: 5376/7200 loss: 1504.2833251953125\n",
            "Samples seen: 5632/7200 loss: 1341.620361328125\n",
            "Samples seen: 5888/7200 loss: 1380.0316162109375\n",
            "Samples seen: 6144/7200 loss: 1454.79638671875\n",
            "Samples seen: 6400/7200 loss: 1213.89990234375\n",
            "Samples seen: 6656/7200 loss: 1541.5146484375\n",
            "Samples seen: 6912/7200 loss: 1463.5947265625\n",
            "Samples seen: 7168/7200 loss: 1308.8138427734375\n",
            "Training loss: 39491.75146484375\n",
            "Mean rank on val set: 402.5885/800\n",
            "Epoch: 4\n",
            "Samples seen: 256/7200 loss: 1199.3990478515625\n",
            "Samples seen: 512/7200 loss: 1391.5570068359375\n",
            "Samples seen: 768/7200 loss: 1274.754638671875\n",
            "Samples seen: 1024/7200 loss: 1379.4176025390625\n",
            "Samples seen: 1280/7200 loss: 1195.059814453125\n",
            "Samples seen: 1536/7200 loss: 1312.7469482421875\n",
            "Samples seen: 1792/7200 loss: 1258.8909912109375\n",
            "Samples seen: 2048/7200 loss: 1184.6593017578125\n",
            "Samples seen: 2304/7200 loss: 1307.197265625\n",
            "Samples seen: 2560/7200 loss: 1081.501708984375\n",
            "Samples seen: 2816/7200 loss: 1301.9569091796875\n",
            "Samples seen: 3072/7200 loss: 1206.298095703125\n",
            "Samples seen: 3328/7200 loss: 1038.535888671875\n",
            "Samples seen: 3584/7200 loss: 1327.21923828125\n",
            "Samples seen: 3840/7200 loss: 1125.56494140625\n",
            "Samples seen: 4096/7200 loss: 1007.3790283203125\n",
            "Samples seen: 4352/7200 loss: 1135.7581787109375\n",
            "Samples seen: 4608/7200 loss: 1074.7933349609375\n",
            "Samples seen: 4864/7200 loss: 1086.6634521484375\n",
            "Samples seen: 5120/7200 loss: 988.6082763671875\n",
            "Samples seen: 5376/7200 loss: 1130.7406005859375\n",
            "Samples seen: 5632/7200 loss: 1140.2442626953125\n",
            "Samples seen: 5888/7200 loss: 889.3334350585938\n",
            "Samples seen: 6144/7200 loss: 1102.142822265625\n",
            "Samples seen: 6400/7200 loss: 1177.63916015625\n",
            "Samples seen: 6656/7200 loss: 1134.949462890625\n",
            "Samples seen: 6912/7200 loss: 1216.907958984375\n",
            "Samples seen: 7168/7200 loss: 1009.7989501953125\n",
            "Training loss: 32679.718322753906\n",
            "Mean rank on val set: 403.47/800\n",
            "Epoch: 5\n",
            "Samples seen: 256/7200 loss: 1036.8040771484375\n",
            "Samples seen: 512/7200 loss: 1116.86083984375\n",
            "Samples seen: 768/7200 loss: 973.6246948242188\n",
            "Samples seen: 1024/7200 loss: 968.5112915039062\n",
            "Samples seen: 1280/7200 loss: 1008.8135986328125\n",
            "Samples seen: 1536/7200 loss: 1103.8782958984375\n",
            "Samples seen: 1792/7200 loss: 1137.4388427734375\n",
            "Samples seen: 2048/7200 loss: 1016.3534545898438\n",
            "Samples seen: 2304/7200 loss: 1037.344482421875\n",
            "Samples seen: 2560/7200 loss: 1165.017333984375\n",
            "Samples seen: 2816/7200 loss: 943.1420288085938\n",
            "Samples seen: 3072/7200 loss: 869.119140625\n",
            "Samples seen: 3328/7200 loss: 926.692626953125\n",
            "Samples seen: 3584/7200 loss: 1159.630126953125\n",
            "Samples seen: 3840/7200 loss: 937.7293701171875\n",
            "Samples seen: 4096/7200 loss: 964.6747436523438\n",
            "Samples seen: 4352/7200 loss: 1026.657470703125\n",
            "Samples seen: 4608/7200 loss: 1059.231689453125\n",
            "Samples seen: 4864/7200 loss: 1000.0465087890625\n",
            "Samples seen: 5120/7200 loss: 909.0159301757812\n",
            "Samples seen: 5376/7200 loss: 1098.2132568359375\n",
            "Samples seen: 5632/7200 loss: 1053.9510498046875\n",
            "Samples seen: 5888/7200 loss: 953.7576293945312\n",
            "Samples seen: 6144/7200 loss: 1022.3099365234375\n",
            "Samples seen: 6400/7200 loss: 988.7069091796875\n",
            "Samples seen: 6656/7200 loss: 997.5657348632812\n",
            "Samples seen: 6912/7200 loss: 1082.33154296875\n",
            "Samples seen: 7168/7200 loss: 969.2271118164062\n",
            "Training loss: 28526.64971923828\n",
            "Mean rank on val set: 404.967/800\n",
            "Epoch: 6\n",
            "Samples seen: 256/7200 loss: 923.500732421875\n",
            "Samples seen: 512/7200 loss: 955.1168823242188\n",
            "Samples seen: 768/7200 loss: 918.3811645507812\n",
            "Samples seen: 1024/7200 loss: 962.777587890625\n",
            "Samples seen: 1280/7200 loss: 935.3043823242188\n",
            "Samples seen: 1536/7200 loss: 883.3682250976562\n",
            "Samples seen: 1792/7200 loss: 934.5069580078125\n",
            "Samples seen: 2048/7200 loss: 878.19873046875\n",
            "Samples seen: 2304/7200 loss: 896.9694213867188\n",
            "Samples seen: 2560/7200 loss: 949.560546875\n",
            "Samples seen: 2816/7200 loss: 840.4006958007812\n",
            "Samples seen: 3072/7200 loss: 885.6866455078125\n",
            "Samples seen: 3328/7200 loss: 791.45849609375\n",
            "Samples seen: 3584/7200 loss: 956.9367065429688\n",
            "Samples seen: 3840/7200 loss: 843.34814453125\n",
            "Samples seen: 4096/7200 loss: 763.6878662109375\n",
            "Samples seen: 4352/7200 loss: 706.2230224609375\n",
            "Samples seen: 4608/7200 loss: 936.2874145507812\n",
            "Samples seen: 4864/7200 loss: 956.8789672851562\n",
            "Samples seen: 5120/7200 loss: 862.773681640625\n",
            "Samples seen: 5376/7200 loss: 815.0813598632812\n",
            "Samples seen: 5632/7200 loss: 923.3309326171875\n",
            "Samples seen: 5888/7200 loss: 805.8143920898438\n",
            "Samples seen: 6144/7200 loss: 837.3531494140625\n",
            "Samples seen: 6400/7200 loss: 822.742919921875\n",
            "Samples seen: 6656/7200 loss: 985.4744262695312\n",
            "Samples seen: 6912/7200 loss: 908.5994873046875\n",
            "Samples seen: 7168/7200 loss: 842.307373046875\n",
            "Training loss: 24722.0703125\n",
            "Mean rank on val set: 403.8145/800\n",
            "Epoch: 7\n",
            "Samples seen: 256/7200 loss: 780.5647583007812\n",
            "Samples seen: 512/7200 loss: 831.5126953125\n",
            "Samples seen: 768/7200 loss: 760.1666259765625\n",
            "Samples seen: 1024/7200 loss: 679.8038330078125\n",
            "Samples seen: 1280/7200 loss: 771.6876831054688\n",
            "Samples seen: 1536/7200 loss: 758.886962890625\n",
            "Samples seen: 1792/7200 loss: 925.7522583007812\n",
            "Samples seen: 2048/7200 loss: 639.5472412109375\n",
            "Samples seen: 2304/7200 loss: 739.8112182617188\n",
            "Samples seen: 2560/7200 loss: 722.1331787109375\n",
            "Samples seen: 2816/7200 loss: 776.681396484375\n",
            "Samples seen: 3072/7200 loss: 717.4357299804688\n",
            "Samples seen: 3328/7200 loss: 723.5512084960938\n",
            "Samples seen: 3584/7200 loss: 756.5794677734375\n",
            "Samples seen: 3840/7200 loss: 679.2369384765625\n",
            "Samples seen: 4096/7200 loss: 804.4110717773438\n",
            "Samples seen: 4352/7200 loss: 819.6677856445312\n",
            "Samples seen: 4608/7200 loss: 650.1427001953125\n",
            "Samples seen: 4864/7200 loss: 778.6236572265625\n",
            "Samples seen: 5120/7200 loss: 724.0598754882812\n",
            "Samples seen: 5376/7200 loss: 730.4114990234375\n",
            "Samples seen: 5632/7200 loss: 650.747314453125\n",
            "Samples seen: 5888/7200 loss: 729.4354858398438\n",
            "Samples seen: 6144/7200 loss: 678.4285888671875\n",
            "Samples seen: 6400/7200 loss: 796.5591430664062\n",
            "Samples seen: 6656/7200 loss: 730.59716796875\n",
            "Samples seen: 6912/7200 loss: 704.05126953125\n",
            "Samples seen: 7168/7200 loss: 730.4271850585938\n",
            "Training loss: 20790.913940429688\n",
            "Mean rank on val set: 404.55175/800\n",
            "Epoch: 8\n",
            "Samples seen: 256/7200 loss: 721.8812255859375\n",
            "Samples seen: 512/7200 loss: 684.3713989257812\n",
            "Samples seen: 768/7200 loss: 639.8015747070312\n",
            "Samples seen: 1024/7200 loss: 696.7061157226562\n",
            "Samples seen: 1280/7200 loss: 679.9229736328125\n",
            "Samples seen: 1536/7200 loss: 702.7518310546875\n",
            "Samples seen: 1792/7200 loss: 818.0610961914062\n",
            "Samples seen: 2048/7200 loss: 588.968505859375\n",
            "Samples seen: 2304/7200 loss: 684.8514404296875\n",
            "Samples seen: 2560/7200 loss: 797.155517578125\n",
            "Samples seen: 2816/7200 loss: 715.769287109375\n",
            "Samples seen: 3072/7200 loss: 741.2064208984375\n",
            "Samples seen: 3328/7200 loss: 576.6690673828125\n",
            "Samples seen: 3584/7200 loss: 629.8269653320312\n",
            "Samples seen: 3840/7200 loss: 625.0740966796875\n",
            "Samples seen: 4096/7200 loss: 633.1817626953125\n",
            "Samples seen: 4352/7200 loss: 661.0977783203125\n",
            "Samples seen: 4608/7200 loss: 600.0992431640625\n",
            "Samples seen: 4864/7200 loss: 596.4362182617188\n",
            "Samples seen: 5120/7200 loss: 567.2387084960938\n",
            "Samples seen: 5376/7200 loss: 690.6312255859375\n",
            "Samples seen: 5632/7200 loss: 725.59912109375\n",
            "Samples seen: 5888/7200 loss: 592.9326171875\n",
            "Samples seen: 6144/7200 loss: 600.1986083984375\n",
            "Samples seen: 6400/7200 loss: 666.53564453125\n",
            "Samples seen: 6656/7200 loss: 678.7351684570312\n",
            "Samples seen: 6912/7200 loss: 699.6319580078125\n",
            "Samples seen: 7168/7200 loss: 619.5512084960938\n",
            "Training loss: 18634.886779785156\n",
            "Mean rank on val set: 405.20225/800\n",
            "Epoch: 9\n",
            "Samples seen: 256/7200 loss: 548.9017333984375\n",
            "Samples seen: 512/7200 loss: 703.2300415039062\n",
            "Samples seen: 768/7200 loss: 556.688720703125\n",
            "Samples seen: 1024/7200 loss: 619.2600708007812\n",
            "Samples seen: 1280/7200 loss: 578.7654418945312\n",
            "Samples seen: 1536/7200 loss: 533.9037475585938\n",
            "Samples seen: 1792/7200 loss: 585.822265625\n",
            "Samples seen: 2048/7200 loss: 560.0054931640625\n",
            "Samples seen: 2304/7200 loss: 648.5584716796875\n",
            "Samples seen: 2560/7200 loss: 567.4223022460938\n",
            "Samples seen: 2816/7200 loss: 508.2599182128906\n",
            "Samples seen: 3072/7200 loss: 542.64453125\n",
            "Samples seen: 3328/7200 loss: 527.047119140625\n",
            "Samples seen: 3584/7200 loss: 638.6402587890625\n",
            "Samples seen: 3840/7200 loss: 560.1481323242188\n",
            "Samples seen: 4096/7200 loss: 509.77130126953125\n",
            "Samples seen: 4352/7200 loss: 504.56390380859375\n",
            "Samples seen: 4608/7200 loss: 575.7724609375\n",
            "Samples seen: 4864/7200 loss: 521.259033203125\n",
            "Samples seen: 5120/7200 loss: 546.6552734375\n",
            "Samples seen: 5376/7200 loss: 514.75830078125\n",
            "Samples seen: 5632/7200 loss: 571.3028564453125\n",
            "Samples seen: 5888/7200 loss: 578.421875\n",
            "Samples seen: 6144/7200 loss: 558.9088745117188\n",
            "Samples seen: 6400/7200 loss: 632.92578125\n",
            "Samples seen: 6656/7200 loss: 545.228759765625\n",
            "Samples seen: 6912/7200 loss: 519.7843627929688\n",
            "Samples seen: 7168/7200 loss: 564.8597412109375\n",
            "Training loss: 15823.510772705078\n",
            "Mean rank on val set: 406.8185/800\n",
            "Training Completed!\n",
            "Loss over epochs\n",
            "[83493.20385742188, 58642.79968261719, 48873.97961425781, 39491.75146484375, 32679.718322753906, 28526.64971923828, 24722.0703125, 20790.913940429688, 18634.886779785156, 15823.510772705078]\n",
            "Mean rank over epochs\n",
            "[402.12875, 401.5795, 400.796, 402.5885, 403.47, 404.967, 403.8145, 404.55175, 405.20225, 406.8185]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSceA2Z6jbKC"
      },
      "source": [
        "###Load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFTn3oNvZ0Li"
      },
      "source": [
        "with open('worddict.pkl', 'rb') as f:\n",
        "\tworddict = pickle.load(f)\n",
        "print('Loaded dictionary')\n",
        "\n",
        "# Loading trained models\n",
        "ImgEncoder = ImgEncoder(dim_image, dim).cuda()\n",
        "ImgEncoder.load_state_dict(torch.load('ImgEncoder.pt'))\n",
        "SentenceEncoder = SentenceEncoder(len(worddict)+2, dim_word, dim).cuda()\n",
        "SentenceEncoder.load_state_dict(torch.load('SentenceEncoder.pt'))\n",
        "print('Models loaded')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
